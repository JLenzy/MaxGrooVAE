{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNtquLL_5VEs"
   },
   "source": [
    "To open this notebook in Colab visit https://goo.gl/magenta/groovae-colab\n",
    "\n",
    "<img src=\"https://magenta-staging.tensorflow.org/assets/groovae/score-groove.png\" alt=\"GrooVAE Figure\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oguz/anaconda3/envs/cc/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/oguz/anaconda3/envs/cc/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/home/oguz/anaconda3/envs/cc/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import librosa\n",
    "import note_seq\n",
    "from note_seq.protobuf import music_pb2\n",
    "\n",
    "from magenta.models.music_vae import configs\n",
    "from magenta.models.music_vae.trained_model import TrainedModel\n",
    "\n",
    "GROOVAE_2BAR_TAP_FIXED_VELOCITY=\"groovae_2bar_tap_fixed_velocity.tar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I47LxktGbwMF",
    "outputId": "c44768cd-e917-4bcc-ba53-6c402d39e972"
   },
   "outputs": [],
   "source": [
    "# If a sequence has notes at time before 0.0, scootch them up to 0\n",
    "def start_notes_at_0(s):\n",
    "    for n in s.notes:\n",
    "        if n.start_time < 0:\n",
    "            n.end_time -= n.start_time\n",
    "            n.start_time = 0\n",
    "    return s\n",
    "\n",
    "# Some midi files come by default from different instrument channels\n",
    "# Quick and dirty way to set midi files to be recognized as drums\n",
    "def set_to_drums(ns):\n",
    "    for n in ns.notes:\n",
    "        n.instrument=9\n",
    "        n.is_drum = True\n",
    "        \n",
    "# quickly change the tempo of a midi sequence and adjust all notes\n",
    "def change_tempo(note_sequence, new_tempo):\n",
    "    new_sequence = copy.deepcopy(note_sequence)\n",
    "    ratio = note_sequence.tempos[0].qpm / new_tempo\n",
    "    for note in new_sequence.notes:\n",
    "        note.start_time = note.start_time * ratio\n",
    "        note.end_time = note.end_time * ratio\n",
    "    new_sequence.tempos[0].qpm = new_tempo\n",
    "    return new_sequence\n",
    "\n",
    "# Calculate quantization steps but do not remove microtiming\n",
    "def quantize(s, steps_per_quarter=4):\n",
    "    return note_seq.sequences_lib.quantize_note_sequence(s,steps_per_quarter)\n",
    "\n",
    "# Destructively quantize a midi sequence\n",
    "def flatten_quantization(s):\n",
    "    beat_length = 60. / s.tempos[0].qpm\n",
    "    step_length = beat_length / 4 #s.quantization_info.steps_per_quarter\n",
    "    new_s = copy.deepcopy(s)\n",
    "    for note in new_s.notes:\n",
    "        note.start_time = step_length * note.quantized_start_step\n",
    "        note.end_time = step_length * note.quantized_end_step\n",
    "    return new_s\n",
    "\n",
    "# Calculate how far off the beat a note is\n",
    "def get_offset(s, note_index):\n",
    "    q_s = flatten_quantization(quantize(s))\n",
    "    true_onset = s.notes[note_index].start_time\n",
    "    quantized_onset = q_s.notes[note_index].start_time\n",
    "    diff = quantized_onset - true_onset\n",
    "    beat_length = 60. / s.tempos[0].qpm\n",
    "    step_length = beat_length / 4#q_s.quantization_info.steps_per_quarter\n",
    "    offset = diff/step_length\n",
    "    return offset\n",
    "\n",
    "def add_silent_note(note_sequence, num_bars):\n",
    "    tempo = note_sequence.tempos[0].qpm\n",
    "    length = 60/tempo * 4 * num_bars\n",
    "    note_sequence.notes.add(\n",
    "        instrument=9, pitch=42, velocity=0, start_time=length-0.02, \n",
    "        end_time=length-0.01, is_drum=True)\n",
    "\n",
    "def is_4_4(s):\n",
    "    ts = s.time_signatures[0]\n",
    "    return (ts.numerator == 4 and ts.denominator ==4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 17:10:26.331855: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n",
      "2022-03-15 17:10:32.333899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-15 17:10:32.344283: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-03-15 17:10:32.344316: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-03-15 17:10:32.344845: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2039\n"
     ]
    }
   ],
   "source": [
    "# Load MIDI files from GMD with MIDI only (no audio) as a tf.data.Dataset\n",
    "dataset_2bar = tfds.as_numpy(tfds.load(\n",
    "    name=\"groove/2bar-midionly\",\n",
    "    split=tfds.Split.VALIDATION,\n",
    "    try_gcs=True))\n",
    "\n",
    "dev_sequences = [quantize(note_seq.midi_to_note_sequence(features[\"midi\"])) for features in dataset_2bar]\n",
    "_ = [set_to_drums(s) for s in dev_sequences]\n",
    "dev_sequences = [s for s in dev_sequences if is_4_4(s) and len(s.notes) > 0 and s.notes[-1].quantized_end_step > note_seq.steps_per_bar_in_quantized_sequence(s)]\n",
    "print(len(dev_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features in dataset_2bar:\n",
    "    x=note_seq.midi_to_note_sequence(features[\"midi\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sequences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPJuKYs0u7f4"
   },
   "source": [
    "# Tap2Drum: Generate a beat from any rhythm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxkHBd_uGFNP"
   },
   "source": [
    "While the Groove model works by removing the micro-timing and velocity information and learning to predict them from just the drum pattern, we can also go in the opposite direction.  Here, we take a representation of a Groove as input (in the form of a rhythm that can have precise timing but where drum categories are ignored) - and then generate drum beats that match the groove implied by this rhythm.  We trained this model by collapsing all drum hits from each beat in the training data to a single \"tapped\" rhythm, and then learning to decode full beats from that rhythm.  This allows us to input any rhythm we like through the precise onset timings in a \"tap\" and let the model decode our rhythm into a beat. We can even simply record taps as audio, or extract them from a recording of another instrument, rather than needing a midi controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some configs to be used later\n",
    "dc_tap = configs.CONFIG_MAP['groovae_2bar_tap_fixed_velocity'].data_converter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Control temperature with OSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "au0HjIkZuBW4",
    "outputId": "9b75e203-a51d-4663-abad-54442617aa3c"
   },
   "outputs": [],
   "source": [
    "# quick method for turning a drumbeat into a tapped rhythm\n",
    "def get_tapped_2bar(s, velocity=85, ride=False):\n",
    "    new_s = dc_tap.from_tensors(dc_tap.to_tensors(s).inputs)[0]\n",
    "    new_s = change_tempo(new_s, s.tempos[0].qpm)\n",
    "    if velocity != 0:\n",
    "        for n in new_s.notes:\n",
    "            n.velocity = velocity\n",
    "    if ride:\n",
    "        for n in new_s.notes:\n",
    "            n.pitch = 42\n",
    "    return new_s\n",
    "\n",
    "def drumify(s, model, temperature=1.0): \n",
    "    encoding, mu, sigma = model.encode([s])\n",
    "    decoded = model.decode(encoding, length=32, temperature=temperature)\n",
    "    return decoded[0]\n",
    "\n",
    "#def make_tap_sequence(tempo, onset_times, onset_frames, onset_velocities,\n",
    "#                     velocity_threshold, start_time, end_time):\n",
    "#    note_sequence = music_pb2.NoteSequence()\n",
    "#    note_sequence.tempos.add(qpm=tempo)\n",
    "#    for onset_vel, onset_time in zip(onset_velocities, onset_times):\n",
    "#        if onset_vel > velocity_threshold and onset_time >= start_time and onset_time < end_time:  # filter quietest notes\n",
    "#            note_sequence.notes.add(\n",
    "#            instrument=9, pitch=42, is_drum=True,\n",
    "#            velocity=onset_vel,  # model will use fixed velocity here\n",
    "#            start_time=onset_time - start_time,\n",
    "#            end_time=onset_time -start_time + 0.01\n",
    "#            )\n",
    "#    return note_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtEPAgNISX5g"
   },
   "source": [
    "Here are a couple of examples using MIDI rhythms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "jFbxCIHRVSDl",
    "outputId": "73cb93d1-bd83-4b08-f256-47cd9078694b"
   },
   "outputs": [],
   "source": [
    "groovae_2bar_tap = TrainedModel(config=configs.CONFIG_MAP['groovae_2bar_tap_fixed_velocity'],\n",
    "                                batch_size=1,\n",
    "                                checkpoint_dir_or_path=GROOVAE_2BAR_TAP_FIXED_VELOCITY)\n",
    "\n",
    "sequence_indices = [1111, 366]\n",
    "for i in sequence_indices:\n",
    "    s = start_notes_at_0(dev_sequences[i])\n",
    "    s = change_tempo(get_tapped_2bar(s, velocity=85, ride=True), dev_sequences[i].tempos[0].qpm)\n",
    "    h = change_tempo(drumify(s, groovae_2bar_tap), s.tempos[0].qpm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IO Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quantized_start_step, quantized_end_step  \n",
    "control_changes {\n",
    "  control_number: 4\n",
    "  control_value: 90\n",
    "  is_drum: true\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_list_to_midi_array(max_list, BPM):\n",
    "    beat_dur=60/BPM # in sec\n",
    "    midi_array=[]\n",
    "    for i in range((len(max_list)//3)):\n",
    "        start_step=float(max_list[3*i]) # in beats\n",
    "        end_step=float(max_list[3*i+1]) # in beats\n",
    "        vel=float(max_list[3*i+2])\n",
    "        start_time=start_step*beat_dur\n",
    "        end_time=end_step*beat_dur\n",
    "        midi_array.append([start_time,end_time,vel])\n",
    "    return np.array(midi_array)\n",
    "\n",
    "def make_tap_sequence_(tempo, midi_array, tpq=220):\n",
    "    note_sequence=music_pb2.NoteSequence()\n",
    "    note_sequence.tempos.add(qpm=tempo)\n",
    "    note_sequence.ticks_per_quarter=tpq\n",
    "    note_sequence.time_signatures.add(numerator=4, denominator=4)\n",
    "    note_sequence.key_signatures.add()\n",
    "    for onset_time, offset_time, onset_velocity in midi_array:\n",
    "        if onset_velocity:\n",
    "            note_sequence.notes.add(instrument=9, # Drum MIDI Program number\n",
    "                                    pitch=42, # Constant\n",
    "                                    is_drum=True,\n",
    "                                    velocity=int(onset_velocity),\n",
    "                                    start_time=onset_time,\n",
    "                                    end_time=offset_time)\n",
    "    note_sequence.total_time=2*4*(60/BPM) # 2bars\n",
    "    return note_sequence  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slt=\"0. 0.03 120. 0.0625 0.0925 120. 0.125 0.155 0. 0.1875 0.2175 0. 0.25 0.28 120. 0.3125 0.3425 0. 0.375 0.405 0. 0.4375 0.4675 0. 0.5 0.53 0. 0.5625 0.5925 0. 0.625 0.655 0. 0.6875 0.7175 120. 0.75 0.78 0. 0.8125 0.8425 0. 0.875 0.905 120. 0.9375 0.9675 0. 1. 1.03 120. 1.0625 1.0925 120. 1.125 1.155 0. 1.1875 1.2175 0. 1.25 1.28 0. 1.3125 1.3425 0. 1.375 1.405 0. 1.4375 1.4675 0. 1.5 1.53 0. 1.5625 1.5925 0. 1.625 1.655 0. 1.6875 1.7175 0. 1.75 1.78 0. 1.8125 1.8425 0. 1.875 1.905 0. 1.9375 1.9675 0.\".split(' ')\n",
    "\n",
    "BPM=110\n",
    "midi_array=max_list_to_midi_array(slt, BPM)\n",
    "len(midi_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_sequence=quantize(make_tap_sequence_(BPM, midi_array))\n",
    "#note_sequence=start_notes_at_0(note_sequence)\n",
    "#s=get_tapped_2bar(note_sequence, velocity=85, ride=True)\n",
    "dc_tap.to_tensors(note_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create pretty_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretty_midi(midi_array, BPM):\n",
    "    \"\"\"Creates a pretty midi file from a midi_array.\"\"\"\n",
    "    midi=pretty_midi.PrettyMIDI(initial_tempo=BPM)\n",
    "    cello_program = pretty_midi.instrument_name_to_program('Cello')\n",
    "    cello = pretty_midi.Instrument(program=cello_program)\n",
    "    for onset_time, offset_time, onset_velocity in midi_array:\n",
    "        if onset_velocity>0:\n",
    "            cello.notes.append(\n",
    "                pretty_midi.Note(\n",
    "                    velocity=85,#int(onset_velocity),\n",
    "                    pitch=42,\n",
    "                    start=onset_time,\n",
    "                    end=offset_time\n",
    "                )\n",
    "            )\n",
    "    midi.instruments.append(cello)\n",
    "    return midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'notes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19124/2256915144.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmidi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_pretty_midi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmidi_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBPM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnote_sequence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_notes_at_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_silent_note\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnote_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmidi_to_note_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmidi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mq_note_sequence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnote_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mset_to_drums\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_note_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_19124/3236922936.py\u001b[0m in \u001b[0;36mstart_notes_at_0\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# If a sequence has notes at time before 0.0, scootch them up to 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstart_notes_at_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_time\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'notes'"
     ]
    }
   ],
   "source": [
    "BPM=120\n",
    "midi_array=max_list_to_midi_array(slt, BPM)\n",
    "midi=create_pretty_midi(midi_array, BPM)\n",
    "\n",
    "note_sequence=start_notes_at_0(add_silent_note(note_seq.midi_to_note_sequence(midi),2))\n",
    "q_note_sequence=quantize(note_sequence)\n",
    "set_to_drums(q_note_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_note_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi.instruments[0].notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_sequence=note_seq.midi_to_note_sequence(midi)\n",
    "q_note_sequence=quantize(note_sequence)\n",
    "set_to_drums(q_note_sequence)\n",
    "\n",
    "s=change_tempo(get_tapped_2bar(q_note_sequence, velocity=85, ride=True), q_note_sequence.tempos[0].qpm)\n",
    "h=change_tempo(drumify(s, groovae_2bar_tap), s.tempos[0].qpm)\n",
    "##dc_tap.to_tensors(q_note_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "CNtquLL_5VEs",
    "yPY395Uo-y9f",
    "KLwr71jntKdP",
    "ylJ8BX1cu0Cn",
    "JPJuKYs0u7f4",
    "1rylVpq0vB-3"
   ],
   "name": "GrooVAE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cc",
   "language": "python",
   "name": "cc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
