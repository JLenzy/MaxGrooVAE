{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNtquLL_5VEs"
   },
   "source": [
    "To open this notebook in Colab visit https://goo.gl/magenta/groovae-colab\n",
    "\n",
    "<img src=\"https://magenta-staging.tensorflow.org/assets/groovae/score-groove.png\" alt=\"GrooVAE Figure\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "<ol>\n",
    "  <li>NN to Max </li>\n",
    "  <li>Python Module Everything</li>\n",
    "  <li>Osc Integration</li>\n",
    "  <li>Preserve Microtiming</li>\n",
    "  <li>Control temperature with OSC</li>\n",
    "  <li>What are mu, sigma?</li>\n",
    "  <li>tpq=220</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oguz/anaconda3/envs/cc/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/home/oguz/anaconda3/envs/cc/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import note_seq\n",
    "from note_seq.protobuf import music_pb2\n",
    "\n",
    "from magenta.models.music_vae import configs\n",
    "from magenta.models.music_vae.trained_model import TrainedModel\n",
    "\n",
    "GROOVAE_2BAR_TAP_FIXED_VELOCITY=\"groovae_2bar_tap_fixed_velocity.tar\"\n",
    "\n",
    "VELOCITY=85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I47LxktGbwMF",
    "outputId": "c44768cd-e917-4bcc-ba53-6c402d39e972"
   },
   "outputs": [],
   "source": [
    "# If a sequence has notes at time before 0.0, scootch them up to 0\n",
    "def start_notes_at_0(s):\n",
    "    for n in s.notes:\n",
    "        if n.start_time < 0:\n",
    "            n.end_time -= n.start_time\n",
    "            n.start_time = 0\n",
    "    return s\n",
    "\n",
    "# Some midi files come by default from different instrument channels\n",
    "# Quick and dirty way to set midi files to be recognized as drums\n",
    "def set_to_drums(ns):\n",
    "    for n in ns.notes:\n",
    "        n.instrument=9\n",
    "        n.is_drum = True\n",
    "        \n",
    "# quickly change the tempo of a midi sequence and adjust all notes\n",
    "def change_tempo(note_sequence, new_tempo):\n",
    "    new_sequence = copy.deepcopy(note_sequence)\n",
    "    ratio = note_sequence.tempos[0].qpm / new_tempo\n",
    "    for note in new_sequence.notes:\n",
    "        note.start_time = note.start_time * ratio\n",
    "        note.end_time = note.end_time * ratio\n",
    "    new_sequence.tempos[0].qpm = new_tempo\n",
    "    return new_sequence\n",
    "\n",
    "# Calculate quantization steps but do not remove microtiming\n",
    "def quantize(s, steps_per_quarter=4):\n",
    "    s_=copy.deepcopy(s)\n",
    "    return note_seq.sequences_lib.quantize_note_sequence(s_,steps_per_quarter)\n",
    "\n",
    "# Destructively quantize a midi sequence\n",
    "def flatten_quantization(s):\n",
    "    beat_length = 60. / s.tempos[0].qpm\n",
    "    step_length = beat_length / 4 #s.quantization_info.steps_per_quarter\n",
    "    new_s = copy.deepcopy(s)\n",
    "    for note in new_s.notes:\n",
    "        note.start_time = step_length * note.quantized_start_step\n",
    "        note.end_time = step_length * note.quantized_end_step\n",
    "    return new_s    \n",
    "\n",
    "def add_silent_note(note_sequence, num_bars):\n",
    "    tempo = note_sequence.tempos[0].qpm\n",
    "    length = 60/tempo * 4 * num_bars\n",
    "    note_sequence.notes.add(\n",
    "        instrument=9, pitch=42, velocity=0, start_time=length-0.02, \n",
    "        end_time=length-0.01, is_drum=True)\n",
    "\n",
    "def is_4_4(s):\n",
    "    ts = s.time_signatures[0]\n",
    "    return (ts.numerator == 4 and ts.denominator ==4)\n",
    "\n",
    "# quick method for turning a drumbeat into a tapped rhythm\n",
    "def get_tapped_2bar(s, velocity=VELOCITY, ride=False):\n",
    "    new_s = dc_tap.from_tensors(dc_tap.to_tensors(s).inputs)[0]\n",
    "    new_s = change_tempo(new_s, s.tempos[0].qpm)\n",
    "    if velocity != 0:\n",
    "        for n in new_s.notes:\n",
    "            n.velocity = velocity\n",
    "    if ride:\n",
    "        for n in new_s.notes:\n",
    "            n.pitch = 42\n",
    "    return new_s\n",
    "\n",
    "def drumify(s, model, temperature=1.0): \n",
    "    encoding, mu, sigma = model.encode([s])\n",
    "    decoded = model.decode(encoding, length=32, temperature=temperature)\n",
    "    return decoded[0]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 13:24:12.473343: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n",
      "2022-03-16 13:24:18.936081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-16 13:24:18.945759: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-03-16 13:24:18.945781: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-03-16 13:24:18.946230: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2039\n"
     ]
    }
   ],
   "source": [
    "# Load MIDI files from GMD with MIDI only (no audio) as a tf.data.Dataset\n",
    "dataset_2bar = tfds.as_numpy(tfds.load(\n",
    "                name=\"groove/2bar-midionly\",\n",
    "                split=tfds.Split.VALIDATION,\n",
    "                try_gcs=True))\n",
    "\n",
    "dev_sequences = [quantize(note_seq.midi_to_note_sequence(features[\"midi\"])) for features in dataset_2bar]\n",
    "_ = [set_to_drums(s) for s in dev_sequences]\n",
    "dev_sequences = [s for s in dev_sequences if is_4_4(s) and len(s.notes) > 0 and s.notes[-1].quantized_end_step > note_seq.steps_per_bar_in_quantized_sequence(s)]\n",
    "print(len(dev_sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPJuKYs0u7f4"
   },
   "source": [
    "# Tap2Drum: Generate a beat from any rhythm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxkHBd_uGFNP"
   },
   "source": [
    "While the Groove model works by removing the micro-timing and velocity information and learning to predict them from just the drum pattern, we can also go in the opposite direction.  Here, we take a representation of a Groove as input (in the form of a rhythm that can have precise timing but where drum categories are ignored) - and then generate drum beats that match the groove implied by this rhythm.  We trained this model by collapsing all drum hits from each beat in the training data to a single \"tapped\" rhythm, and then learning to decode full beats from that rhythm.  This allows us to input any rhythm we like through the precise onset timings in a \"tap\" and let the model decode our rhythm into a beat. We can even simply record taps as audio, or extract them from a recording of another instrument, rather than needing a midi controller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtEPAgNISX5g"
   },
   "source": [
    "Here are a couple of examples using MIDI rhythms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "jFbxCIHRVSDl",
    "outputId": "73cb93d1-bd83-4b08-f256-47cd9078694b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Building MusicVAE model with BidirectionalLstmEncoder, GrooveLstmDecoder, and hparams:\n",
      "{'max_seq_len': 32, 'z_size': 256, 'free_bits': 48, 'max_beta': 0.2, 'beta_rate': 0.0, 'batch_size': 1, 'grad_clip': 1.0, 'clip_mode': 'global_norm', 'grad_norm_clip_to_zero': 10000, 'learning_rate': 0.001, 'decay_rate': 0.9999, 'min_learning_rate': 1e-05, 'conditional': True, 'dec_rnn_size': [256, 256], 'enc_rnn_size': [512], 'dropout_keep_prob': 0.3, 'sampling_schedule': 'constant', 'sampling_rate': 0.0, 'use_cudnn': False, 'residual_encoder': False, 'residual_decoder': False, 'control_preprocessing_rnn_size': [256]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Building MusicVAE model with BidirectionalLstmEncoder, GrooveLstmDecoder, and hparams:\n",
      "{'max_seq_len': 32, 'z_size': 256, 'free_bits': 48, 'max_beta': 0.2, 'beta_rate': 0.0, 'batch_size': 1, 'grad_clip': 1.0, 'clip_mode': 'global_norm', 'grad_norm_clip_to_zero': 10000, 'learning_rate': 0.001, 'decay_rate': 0.9999, 'min_learning_rate': 1e-05, 'conditional': True, 'dec_rnn_size': [256, 256], 'enc_rnn_size': [512], 'dropout_keep_prob': 0.3, 'sampling_schedule': 'constant', 'sampling_rate': 0.0, 'use_cudnn': False, 'residual_encoder': False, 'residual_decoder': False, 'control_preprocessing_rnn_size': [256]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\n",
      "Encoder Cells (bidirectional):\n",
      "  units: [512]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\n",
      "Encoder Cells (bidirectional):\n",
      "  units: [512]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\n",
      "Decoder Cells:\n",
      "  units: [256, 256]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\n",
      "Decoder Cells:\n",
      "  units: [256, 256]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "/home/oguz/anaconda3/envs/cc/lib/python3.7/site-packages/magenta/models/music_vae/lstm_utils.py:99: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  name=name),\n",
      "/home/oguz/anaconda3/envs/cc/lib/python3.7/site-packages/keras/legacy_tf_layers/core.py:261: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  return layer.apply(inputs)\n",
      "/home/oguz/anaconda3/envs/cc/lib/python3.7/site-packages/magenta/contrib/rnn.py:751: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  self._names[\"W\"], [input_size + self._num_units, self._num_units * 4])\n",
      "/home/oguz/anaconda3/envs/cc/lib/python3.7/site-packages/magenta/contrib/rnn.py:754: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  initializer=tf.constant_initializer(0.0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/oguz/anaconda3/envs/cc/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/oguz/anaconda3/envs/cc/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/oguz/anaconda3/envs/cc/lib/python3.7/site-packages/magenta/contrib/rnn.py:474: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/oguz/anaconda3/envs/cc/lib/python3.7/site-packages/magenta/contrib/rnn.py:474: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/oguz/anaconda3/envs/cc/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:446: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/oguz/anaconda3/envs/cc/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:446: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/oguz/anaconda3/envs/cc/lib/python3.7/site-packages/tensorflow_probability/python/bijectors/affine_linear_operator.py:116: LinearOperator.graph_parents (from tensorflow.python.ops.linalg.linear_operator) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Do not call `graph_parents`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oguz/anaconda3/envs/cc/lib/python3.7/site-packages/magenta/models/music_vae/base_model.py:199: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  kernel_initializer=tf.random_normal_initializer(stddev=0.001))\n",
      "/home/oguz/anaconda3/envs/cc/lib/python3.7/site-packages/magenta/models/music_vae/base_model.py:205: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  kernel_initializer=tf.random_normal_initializer(stddev=0.001))\n",
      "WARNING:tensorflow:From /home/oguz/anaconda3/envs/cc/lib/python3.7/site-packages/tensorflow_probability/python/bijectors/affine_linear_operator.py:116: LinearOperator.graph_parents (from tensorflow.python.ops.linalg.linear_operator) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Do not call `graph_parents`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unbundling checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unbundling checkpoint.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpzwjjxrkm/groovae_2bar_tap_fixed_velocity/model.ckpt-3668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpzwjjxrkm/groovae_2bar_tap_fixed_velocity/model.ckpt-3668\n"
     ]
    }
   ],
   "source": [
    "dc_tap = configs.CONFIG_MAP['groovae_2bar_tap_fixed_velocity'].data_converter\n",
    "\n",
    "groovae_2bar_tap = TrainedModel(config=configs.CONFIG_MAP['groovae_2bar_tap_fixed_velocity'],\n",
    "                                batch_size=1,\n",
    "                                checkpoint_dir_or_path=GROOVAE_2BAR_TAP_FIXED_VELOCITY)\n",
    "\n",
    "sequence_indices = [1111, 366]\n",
    "for i in sequence_indices:\n",
    "    s = start_notes_at_0(dev_sequences[i])\n",
    "    s = change_tempo(get_tapped_2bar(s, velocity=85, ride=True), dev_sequences[i].tempos[0].qpm)\n",
    "    h = change_tempo(drumify(s, groovae_2bar_tap), s.tempos[0].qpm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IO Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quantized_start_step, quantized_end_step  \n",
    "control_changes {\n",
    "  control_number: 4\n",
    "  control_value: 90\n",
    "  is_drum: true\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_list_to_midi_array(max_list, BPM, n_bars=2, n_steps_per_quarter_note=4):\n",
    "    \"\"\"max_list timing are in bars. Assumes 4/4 timing\"\"\"\n",
    "    assert len(max_list)==3*(n_bars*4*n_steps_per_quarter_note), 'List length wrong!'\n",
    "    beat_dur=60/BPM # in sec\n",
    "    midi_array=[]\n",
    "    for i in range((len(max_list)//3)):\n",
    "        start_step=4*float(max_list[3*i]) # in beats\n",
    "        end_step=4*float(max_list[3*i+1]) # in beats\n",
    "        vel=float(max_list[3*i+2])\n",
    "        start_time=start_step*beat_dur\n",
    "        end_time=end_step*beat_dur\n",
    "        midi_array.append([start_time,end_time,vel])\n",
    "    return np.array(midi_array)\n",
    "\n",
    "def make_tap_sequence(midi_array, tempo, tpq=480):\n",
    "    note_sequence=music_pb2.NoteSequence()\n",
    "    note_sequence.tempos.add(qpm=tempo)\n",
    "    note_sequence.ticks_per_quarter=tpq\n",
    "    note_sequence.time_signatures.add(numerator=4, denominator=4)\n",
    "    note_sequence.key_signatures.add()\n",
    "    for onset_time, offset_time, onset_velocity in midi_array:\n",
    "        if onset_velocity: # Non-zero velocity notes only\n",
    "            note_sequence.notes.add(instrument=9, # Drum MIDI Program number\n",
    "                                    pitch=42, # Constant\n",
    "                                    is_drum=True,\n",
    "                                    velocity=VELOCITY,\n",
    "                                    start_time=onset_time,\n",
    "                                    end_time=offset_time)\n",
    "    note_sequence.total_time=2*4*(60/BPM) # 2bars\n",
    "    return note_sequence \n",
    "\n",
    "def quantize_to_beat_divisions(beat, division=32):\n",
    "    \"\"\"Quantize a floating point beat? to a 1/division'th beat\"\"\"\n",
    "    if division!=1: \n",
    "        return (beat//(1/division))*(1/division)\n",
    "    else: # do not quantize\n",
    "        return beat\n",
    "\n",
    "def NN_output_to_Max(h, BPM, pre_quantization=False, beat_quantization_division=1):\n",
    "    \"\"\"Return in [beats]\"\"\"\n",
    "    _h=copy.deepcopy(h)\n",
    "    beat_dur=60/BPM\n",
    "    if pre_quantization:\n",
    "        _h=quantize(_h)\n",
    "    midi_array=[]\n",
    "    for note in _h.notes:\n",
    "        start=quantize_to_beat_divisions(note.start_time/beat_dur, beat_quantization_division)\n",
    "        dur=quantize_to_beat_divisions((note.end_time-note.start_time)/beat_dur, beat_quantization_division)\n",
    "        midi_array.append([start, dur, note.velocity, note.pitch])\n",
    "    midi_array=np.array(midi_array)\n",
    "    return midi_array     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lst=\"0. 0.03 120. 0.0625 0.0925 120. 0.125 0.155 120. 0.1875 0.2175 120. 0.25 0.28 120. 0.3125 0.3425 120. 0.375 0.405 0. 0.4375 0.4675 0. 0.5 0.53 0. 0.5625 0.5925 0. 0.625 0.655 0. 0.6875 0.7175 120. 0.75 0.78 0. 0.8125 0.8425 0. 0.875 0.905 120. 0.9375 0.9675 0. 1. 1.03 120. 1.0625 1.0925 120. 1.125 1.155 0. 1.1875 1.2175 0. 1.25 1.28 0. 1.3125 1.3425 0. 1.375 1.405 0. 1.4375 1.4675 0. 1.5 1.53 0. 1.5625 1.5925 0. 1.625 1.655 0. 1.6875 1.7175 0. 1.75 1.78 120. 1.8125 1.8425 120. 1.875 1.905 120. 1.9375 1.9675 120.\".split(' ')\n",
    "\n",
    "BPM=120\n",
    "# Convert max list to a readable format\n",
    "midi_array=max_list_to_midi_array(max_lst, BPM)\n",
    "len(midi_array)\n",
    "# Convert it into the pre-NN input format\n",
    "note_sequence=make_tap_sequence(midi_array, BPM)\n",
    "note_sequence=quantize(note_sequence)\n",
    "set_to_drums(note_sequence)\n",
    "# Convert to NN input format\n",
    "s=copy.deepcopy(note_sequence)\n",
    "s=start_notes_at_0(s)\n",
    "s=change_tempo(get_tapped_2bar(s, velocity=VELOCITY, ride=True), BPM)\n",
    "assert BPM==s.tempos[0].qpm, 'Tempo conversion failed at tapped bar creation'\n",
    "# Get NN prediction\n",
    "h=change_tempo(drumify(s, groovae_2bar_tap), BPM)\n",
    "assert BPM==h.tempos[0].qpm, 'Tempo conversion failed at NN creation'\n",
    "# Convert to Max array\n",
    "MAX_array=NN_output_to_Max(h, BPM, beat_quantization_division=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.     ,  0.25   , 17.     , 38.     ],\n",
       "       [ 0.21875,  0.25   , 57.     , 38.     ],\n",
       "       [ 0.21875,  0.25   , 28.     , 50.     ],\n",
       "       [ 0.46875,  0.25   , 57.     , 38.     ],\n",
       "       [ 0.75   ,  0.25   , 18.     , 36.     ],\n",
       "       [ 0.71875,  0.25   , 45.     , 38.     ],\n",
       "       [ 0.96875,  0.25   , 51.     , 38.     ],\n",
       "       [ 1.25   ,  0.25   , 12.     , 36.     ],\n",
       "       [ 1.96875,  0.25   , 26.     , 38.     ],\n",
       "       [ 2.75   ,  0.25   ,  6.     , 48.     ],\n",
       "       [ 2.96875,  0.25   , 30.     , 38.     ],\n",
       "       [ 3.25   ,  0.25   , 45.     , 38.     ],\n",
       "       [ 3.5    ,  0.25   , 36.     , 38.     ],\n",
       "       [ 3.96875,  0.25   , 47.     , 36.     ],\n",
       "       [ 6.     ,  0.25   , 45.     , 38.     ],\n",
       "       [ 6.25   ,  0.25   , 21.     , 38.     ],\n",
       "       [ 6.5    ,  0.25   , 11.     , 38.     ],\n",
       "       [ 6.71875,  0.25   ,  4.     , 36.     ],\n",
       "       [ 6.71875,  0.25   , 25.     , 38.     ],\n",
       "       [ 6.96875,  0.25   , 44.     , 38.     ],\n",
       "       [ 7.25   ,  0.25   , 23.     , 38.     ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "CNtquLL_5VEs",
    "yPY395Uo-y9f",
    "KLwr71jntKdP",
    "ylJ8BX1cu0Cn",
    "JPJuKYs0u7f4",
    "1rylVpq0vB-3"
   ],
   "name": "GrooVAE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cc",
   "language": "python",
   "name": "cc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
